# Prefect Data Cleaning Workflows for Uganda District Data

## Project Overview
Build a Prefect-based data cleaning pipeline for Uganda district data including facilities (health, education), demographic data, economic indicators, and other location-aggregated datasets. The pipeline should handle diverse CSV structures while maintaining consistent location hierarchies and standardized outputs.

## Updated Project Structure
```
uganda-district-data-cleaning/
├── flows/
│   ├── facility_cleaning_flow.py
│   ├── aggregated_data_cleaning_flow.py
│   └── location_extraction_flow.py
├── tasks/
│   ├── csv_cleaning.py
│   ├── location_processing.py
│   ├── facility_processing.py
│   ├── aggregated_data_processing.py
│   └── validation.py
├── data/
│   ├── raw/
│   │   ├── facilities/
│   │   │   ├── health/
│   │   │   │   ├── kayunga_health_facilities.csv
│   │   │   │   └── masindi_health_facilities.csv
│   │   │   └── education/
│   │   │       └── kayunga_schools.csv
│   │   ├── demographics/
│   │   │   ├── kayunga_population_by_subcounty.csv
│   │   │   └── masindi_age_distribution.csv
│   │   ├── economic/
│   │   │   └── kayunga_market_centers.csv
│   │   └── infrastructure/
│   │       └── kayunga_water_points.csv
│   └── processed/
│       ├── facilities/
│       ├── aggregated_data/
│       ├── locations/
│       │   ├── master_locations.json
│       │   └── district_locations/
│       └── logs/
├── config/
│   ├── data_type_rules.yaml
│   └── location_mappings.yaml
└── requirements.txt
```

## Core Processing Types

### 1. Facility Data Processing
- Individual records per facility
- Requires facility_id generation
- Full location hierarchy (District → Subcounty → Parish → Village)
- Coordinate validation for point locations

### 2. Aggregated Data Processing  
- Summary data by location (subcounty population, parish water access, etc.)
- No facility_id needed
- Location-based grouping and validation
- Different validation rules (totals should sum correctly, percentages within bounds)

### 3. Location Hierarchy Processing
- Extract and standardize location names across all data types
- Generate consistent location codes
- Build unified location reference files
- Handle location name variations and conflicts

## Enhanced Cleaning Requirements

### 1. Data Type Detection (from filename/content)
```python
# Auto-detect patterns
facility_indicators = ['facility', 'health', 'school', 'clinic', 'hospital']
aggregated_indicators = ['population', 'demographic', 'economic', 'summary']
thematic_area_mapping = {
    'health': ['health', 'medical', 'clinic', 'hospital'],
    'education': ['school', 'education', 'learning'],
    'water': ['water', 'borehole', 'well'],
    'demographics': ['population', 'census', 'demographic']
}
```

### 2. Flexible Location Processing
- Handle datasets with partial location hierarchy (only District+Subcounty)
- Map location name variations to standard names
- Build location relationships from multiple data sources
- Validate aggregated totals match sub-location sums

### 3. Validation Rules by Data Type
- **Facilities:** Required coordinates, unique facility names per location
- **Aggregated:** Totals consistency, percentage bounds (0-100%), population reasonableness
- **All types:** Location hierarchy consistency, district name standardization

## Key Task Categories

### Universal Cleaning Tasks
1. `detect_data_type()` - Classify as facility vs aggregated data
2. `detect_thematic_area()` - Auto-identify from filename/content  
3. `standardize_headers()` - Clean column names consistently
4. `clean_location_data()` - Standardize location name formats
5. `validate_location_hierarchy()` - Check hierarchy consistency

### Facility-Specific Tasks
1. `generate_facility_ids()` - Create unique facility identifiers
2. `validate_coordinates()` - Check lat/lng within district bounds
3. `detect_duplicate_facilities()` - Flag exact/similar facility matches

### Aggregated Data Tasks
1. `validate_aggregation_totals()` - Check sum consistency
2. `validate_percentage_bounds()` - Ensure percentages are 0-100%
3. `cross_reference_locations()` - Validate against known locations

### Location Master Tasks
1. `extract_all_locations()` - Build comprehensive location list
2. `resolve_location_conflicts()` - Handle name variations
3. `generate_unified_location_codes()` - Ensure unique codes across districts
4. `update_master_locations()` - Merge with existing location data

## Enhanced Configuration

### data_type_rules.yaml
```yaml
data_types:
  facility:
    required_columns: ['name', 'location']
    optional_columns: ['latitude', 'longitude', 'contact']
    validation:
      coordinates: true
      unique_names: true
  
  aggregated:
    required_columns: ['location']
    validation:
      sum_consistency: true
      percentage_bounds: true

thematic_detection:
  health: ['health', 'medical', 'clinic', 'hospital', 'hc']
  education: ['school', 'education', 'learning', 'university']
  water: ['water', 'borehole', 'well', 'source']
  demographics: ['population', 'census', 'demographic', 'people']
```

## Sample Enhanced Flow
```python
@flow(name="district-data-cleaning")
def clean_district_data(input_file: str, district: str):
    # Auto-detect data type and thematic area
    # Apply appropriate cleaning workflow
    # Extract/validate location hierarchy  
    # Generate standardized outputs
    # Update master location files
    # Create data quality report
```

## Expected Outputs by Data Type

### For Facility Data:
- Cleaned CSV with facility_id, standardized locations, coordinates
- Updated location hierarchy files

### For Aggregated Data:
- Cleaned CSV with standardized location codes and validated totals
- Location mapping report for any unmatched locations

### For All Data:
- Data quality report with type-specific metrics
- Processing logs with workflow details
- Updated master_locations.json with new/validated locations

Ready for Claude Code implementation.